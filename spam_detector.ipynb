{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_directory = \"train/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_dictionary = {}    # stores words in ham files and their frequencies\n",
    "spam_dictionary = {}    # stores words in spam files and their frequencies\n",
    "vocabulary = set()    # stores unique words present in all files (spam and ham)\n",
    "\n",
    "def word_count_directory(train_data_directory):\n",
    "    \n",
    "    # list of file paths for files in train_data_directory\n",
    "    file_list = [os.path.join(train_data_directory,f) for f in os.listdir(train_data_directory)]\n",
    "    \n",
    "    # intialize no of spam and ham files\n",
    "    no_of_spam_files = 0\n",
    "    no_of_ham_files = 0\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        with open(file_path,encoding='latin-1') as infile:\n",
    "            # to store type of file 'spam' or 'ham'\n",
    "            file_type = ''\n",
    "            if 'spam' in file_path:\n",
    "                file_type = 'spam'\n",
    "                no_of_spam_files += 1\n",
    "            elif 'ham' in file_path:\n",
    "                file_type = 'ham'\n",
    "                no_of_ham_files += 1\n",
    "                \n",
    "            # Loop through each line of the file \n",
    "            for line in infile:\n",
    "                 \n",
    "                line = line.strip()    # Remove the leading spaces and newline character\n",
    "                lower_line = str.lower(line)    # Convert characters in line to lowercase to avoid case mismatch\n",
    "                valid_words = re.split('[^a-zA-Z]',lower_line) # filter words following the given regex\n",
    "                valid_words = list(filter(None, valid_words))   # filter words with length greater than 0\n",
    "                \n",
    "                # Iterate over each word in line \n",
    "                for word in valid_words:\n",
    "                    if file_type == 'ham':\n",
    "                        # Check if the word is already in dictionary\n",
    "                        if word in ham_dictionary:\n",
    "                            ham_dictionary[word] += 1\n",
    "                        else:\n",
    "                            ham_dictionary[word] = 1     # add word to dictionary with count 1\n",
    "                            vocabulary.add(word)     # add word to vocabulary set\n",
    "\n",
    "                            # if this word is not present in spam_dictionary, add it with count 0\n",
    "                            if word not in spam_dictionary:\n",
    "                                spam_dictionary[word] = 0\n",
    "\n",
    "                    elif file_type == 'spam':\n",
    "                        # Check if the word is already in dictionary\n",
    "                        if word in spam_dictionary:\n",
    "                            spam_dictionary[word] += 1\n",
    "                        else:\n",
    "                            spam_dictionary[word] = 1    # add word to dictionary with count 1\n",
    "                            vocabulary.add(word)    # add word to vocabulary set\n",
    "\n",
    "                            # if this word is not present in ham_dictionary, add it with count 0\n",
    "                            if word not in ham_dictionary:\n",
    "                                ham_dictionary[word] = 0\n",
    "    return no_of_spam_files,no_of_ham_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_spam_files, no_of_ham_files = word_count_directory(train_data_directory)\n",
    "total_no_of_files = no_of_spam_files + no_of_ham_files\n",
    "prior_prob_of_spam = no_of_spam_files / total_no_of_files\n",
    "prior_prob_of_ham = no_of_ham_files / total_no_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary,ham_dictionary,spam_dictionary):\n",
    "    \n",
    "    vocabulary = sorted(vocabulary)    # sorting the vocabulary to maintain order in model.txt\n",
    "    f = open(\"model.txt\",\"w+\")    # creating file that would store the model\n",
    "    N = len(vocabulary)    # getting size of vocabulary\n",
    "    delta = 0.5    # smoothing value\n",
    "    \n",
    "    smoothed_N = (delta * N)\n",
    "    # calculating smoothed denominator for calculating condinational probability of ham words\n",
    "    ham_denominator = sum(ham_dictionary.values()) + smoothed_N\n",
    "    \n",
    "    # calculating smoothed denominator for calculating condinational probability of spam words\n",
    "    spam_denominator = sum(spam_dictionary.values()) + smoothed_N\n",
    "    \n",
    "    for i,word in enumerate(vocabulary):\n",
    "        \n",
    "        freq_in_ham = ham_dictionary[word]    # frequency of word in ham dictionary\n",
    "        c_p_in_ham = (freq_in_ham + delta) / ham_denominator    # conditional probabiltiy of word in ham\n",
    "        freq_in_spam = spam_dictionary[word]    # frequency of word in spam dictionary\n",
    "        c_p_in_spam = (freq_in_spam + delta) / spam_denominator    # conditional probabiltiy of word in spam\n",
    "        \n",
    "        ham_dictionary[word] = c_p_in_ham\n",
    "        spam_dictionary[word] = c_p_in_spam\n",
    "        \n",
    "        # writing all data to model.txt\n",
    "        f.write(str(i+1)+'  '+word+'  '+str(freq_in_ham)+'  '+str( \"{:.8f}\".format(float( c_p_in_ham )) )+'  '+str(freq_in_spam)+'  '+str( \"{:.8f}\".format(float( c_p_in_spam )) )+'\\n')\n",
    "    \n",
    "    f.close()    # closing the file\n",
    "    \n",
    "create_model(vocabulary,ham_dictionary,spam_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_directory = \"test/\"\n",
    "\n",
    "# applying log10 on prior probabilities\n",
    "log_of_ham = math.log10(prior_prob_of_ham)\n",
    "log_of_spam = math.log10(prior_prob_of_spam)\n",
    "\n",
    "# initialising variables needed for confusion matrix\n",
    "true_positive = 0   # correct Ham -> result Ham\n",
    "true_negative = 0    # correct Spam -> result Spam\n",
    "false_positive = 0    # correct Spam -> result Ham\n",
    "false_negative = 0    # correct Ham -> result Spam\n",
    "\n",
    "file_list = [os.path.join(test_data_directory,f) for f in os.listdir(test_data_directory)]    # file paths of test files\n",
    "temp_counter = 0    # counter to store the test file count\n",
    "\n",
    "f = open(\"result.txt\", \"w+\")   # 'w+' for reading and writing\n",
    "f.truncate(0)\n",
    "\n",
    "for file_path in file_list:\n",
    "    \n",
    "    with open(file_path,encoding = 'latin-1') as infile:\n",
    "\n",
    "        file_name = file_path.rsplit('/',1)[1]    # file name to store in result.txt\n",
    "        temp_counter = temp_counter + 1\n",
    "        score_log_ham = log_of_ham     # score for ham\n",
    "        score_log_spam = log_of_spam    # score for spam\n",
    "\n",
    "        if(\"test-ham\" in file_path):\n",
    "            correct_classification = \"ham\"\n",
    "        else:\n",
    "            correct_classification = \"spam\"\n",
    "\n",
    "        vocab_test = []    #  stores words in test file\n",
    "        for line in infile:\n",
    "\n",
    "            line = line.strip()    # Remove the leading spaces and newline character\n",
    "            lower_line = str.lower(line)    # Convert characters in line to lowercase to avoid case mismatch\n",
    "            valid_words = re.split('[^a-zA-Z]',lower_line) # filter words following the given regex\n",
    "            valid_words = list(filter(None, valid_words))   # filter words with length greater than 0\n",
    "            vocab_test = vocab_test + valid_words    # appending valid_words to vocab_test\n",
    "\n",
    "\n",
    "        for word in vocab_test:\n",
    "            if word in vocabulary:\n",
    "                # add log10 of conditional probability of word in ham_dictionary\n",
    "                score_log_ham = score_log_ham + math.log10(ham_dictionary[word])\n",
    "                # add log10 of conditional probability of word in ham_dictionary\n",
    "                score_log_spam = score_log_spam + math.log10(spam_dictionary[word])\n",
    "        \n",
    "\n",
    "        if(score_log_ham > score_log_spam):\n",
    "            predicted_classification = \"ham\"\n",
    "        else:\n",
    "            predicted_classification = \"spam\"\n",
    "\n",
    "        if(correct_classification == predicted_classification):\n",
    "            label = \"right\"\n",
    "        else:\n",
    "            label = \"wrong\"\n",
    "\n",
    "        if(correct_classification == \"ham\" and predicted_classification == \"ham\"):\n",
    "            true_positive = true_positive + 1\n",
    "            \n",
    "        elif(correct_classification == \"spam\" and predicted_classification == \"spam\"):\n",
    "            true_negative = true_negative + 1\n",
    "            \n",
    "        elif(correct_classification == \"spam\" and predicted_classification == \"ham\"):\n",
    "            false_positive = false_positive + 1\n",
    "            \n",
    "        elif(correct_classification == \"ham\" and predicted_classification == \"spam\"):\n",
    "            false_negative = false_negative + 1\n",
    "\n",
    "        # format scores to appropriate string value\n",
    "        score_log_ham = str( \"{:.8f}\".format(float(score_log_ham)))\n",
    "        score_log_spam = str( \"{:.8f}\".format(float(score_log_spam)))\n",
    "        \n",
    "        # writing results to result.txt\n",
    "        f.write(str(str(temp_counter)+\"  \"+str(file_name)+\"  \"+str(predicted_classification)+\"  \"+str(score_log_ham)+\"  \"+str(score_log_spam)+\"  \"+str(correct_classification)+\"  \"+str(label)+\"\\n\"))\n",
    "\n",
    "f.close()    # closing file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_true_positive = true_negative\n",
    "spam_true_negative = true_positive\n",
    "spam_false_positive = false_negative\n",
    "spam_false_negative = false_positive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
